# NLP Assignment 1
The proposed model is a multi layer, 1D convolution network to perform regression on California housing dataset with PyTorch in Google Colab. A modified dataset of California housing (longitude, latitude, housing_median_age, total_rooms, total_bedroom, population, household, median_income and median_house_value) is downloaded and uploaded onto the drive so that it can be fetched on to the code. The model is split 70:30 for training and testing respectively using the utility ‘model_selection’ from sklearn library. The model is then processed to remove any incomplete entries. The column ‘median_house_value’ is being predicted by using the other columns from the dataset. The class defined for initialization method will be a sub class to the ‘torch.nn.Module’ class which is the base class for all neural network classes. 

      In the initialization method, we define the input layer and store the parameters for it. Kernel size, which is the number of filters in the convolutional layer, is defined along with the max pooling layer. Max pooling layer is used to reduce the total number of layers. There is also a average pooling layer, which isn’t the best for this model and hence max pooling layer is used. Flatten layer is defined, which is used to convert the data into a 1D array so that it can be passed as an input to the next layer.  An activation function, ReLU (Rectified Linear Unit) is used to output the input directly if positive or output zero, if negative. 

      Each feature of the dataset is unique and can be represented in many ways. One such way is a histogram plot. To optimize the model, Stochastic Gradient Descent (SGD) is used wherein, it takes the learning rate and the parameter that has to be optimized as its parameter to perform updates. The model makes use of L1Loss function to reduce the loss and R2Score, which is the coefficient of determination, is the performance measure to determine the closeness of data to the regression line. Batch size impacts the efficiency of training and also the noise level of the gradient estimate. For this model, the batch size is maintained at 64 after comparing with that of 128 size.  

A method is created to allow concurrent running of the batches through the model. The average L1 Loss function is procured by first, getting the prediction of the training set from the model, calculate the loss and then get the R^2 score. It is important to clear the errors while training so that they don’t accumulate.

      The main part of the model lies in the architecture of CNN where it has two layers which are run through the ReLU layer. ReLU is preferred over other activation function such as tanh or sigmoid because the functions tend to saturate i.e., largest value tends to 1 and smallest value tends to -1 for tanh and 0 for sigmoid. This drawback is called as the vanishing gradient problem. ReLU, being a non linear activation function saturates only uni-directionally and hence, the vanishing gradient problem reduces drastically. 
      To have the model trained, the epochs are defined to 10. For the defined epoch, the model goes through the batches and gets the average loss. 
