# -*- coding: utf-8 -*-
"""NLP_Assignment1_1106937.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_gzPLSEtnra99dyngL3W7-ClOGT6j--j
"""

#Importing the pandas library to read our dataset
import pandas as pd

#Getting the train/test split package from sklearn for preparing our dataset to test and train the model with
from sklearn.model_selection import train_test_split

#Importing the numpy library to work with and manipulate the data
import numpy as np

#AssignmentQuestion1 a
#Reads a comma separated value (CSV) file
dataset = pd.read_csv('/content/drive/My Drive/SampleData/housing.csv')

#The path to your .csv file

dataset = dataset.dropna()

#View the first five rows of the dataset
print("Here are the first Ten records of the dataset:")

dataset.head(10)

dataset.describe()

#AssignmentQuestion 1 b
#importing pandas
import pandas as pd
#importing pyplot module from matplotlib
from matplotlib import pyplot as plt
#using seaborn style
plt.style.use('seaborn')

#setting the values into a variable for each column
longitude = dataset['longitude']
latitude = dataset['latitude']
housing_median_age = dataset['housing_median_age']
total_rooms = dataset['total_rooms']
total_bedrooms = dataset['total_bedrooms']
population = dataset['population']
households = dataset['households']
median_income = dataset['median_income']
median_house_value = dataset['median_house_value']
ocean_proximity = dataset['ocean_proximity']

#subplots creates a figure and by default has axis for rows and columns to 1 

#Creating an image for each subplots
fig, (ax1,ax2,ax3,ax4,ax5,ax6,ax7,ax8,ax9) = plt.subplots(9)

#fig, ax1 = plt.subplots()
#fig, ax2 = plt.subplots()
#fig, ax3 = plt.subplots()
#fig, ax4 = plt.subplots()
#fig, ax5 = plt.subplots()
#fig, ax6 = plt.subplots()
#fig, ax7 = plt.subplots()
#fig, ax8 = plt.subplots()
#fig, ax9 = plt.subplots()
#fig, ax10 = plt.subplots()



#Providing the subplot with a label and color
ax1.plot(longitude, label = 'longitude', color='yellow')
ax2.plot(latitude, label = 'latitude', color='red')
ax3.plot(housing_median_age, label = 'housing_median_age', color='blue')
ax4.plot(total_rooms, label = 'total_rooms', color='green')
ax5.plot(total_bedrooms, label = 'total_bedrooms', color='Yellow')
ax6.plot(population, label = 'population', color='orange')
ax7.plot(households, label = 'households', color='white')
ax8.plot(median_income, label = 'median_income', color='brown')
ax9.plot(median_house_value, label = 'median_house_value', color='yellow')
#ax10.plot(ocean_proximity, label = 'ocean_proximity', color='pink')

#Giving legend details
ax1.legend()
#ax1.set_ylabel('longitude')
ax2.legend()
#ax2.set_ylabel('latitude')
ax3.legend()
#ax3.set_ylabel('Median house age')
ax4.legend()
#ax4.set_ylabel('total number of rooms')
ax5.legend()
#ax5.set_ylabel('total number of bedrooms')
ax6.legend()
#ax6.set_ylabel('total population')
ax7.legend()
#ax7.set_ylabel('total number of people per house')
ax8.legend()
#ax8.set_ylabel('median income')
ax9.legend()

ax9.set_xlabel('Count')


#automatic padding for the plot
#plt.tight_layout()

#saving the image as .png
fig.savefig('fig.png')
#show the plot
plt.show()

#AssignmentQuestion1 C
#Predicting the Median house value
Y = dataset['median_house_value']

#Y is predicted by other remaining columns
#selecting the Median_Income column from the longitude column
X = dataset.loc[:,'longitude':'median_income']

#X

#Splitting the dataset such that 30% is for testing and the remaining 70% for Training
x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3)

#Converting the datasets to numpy arrays to work with the model
x_train_np = x_train.to_numpy()
y_train_np = y_train.to_numpy()

#Converting the testing data
x_test_np = x_test.to_numpy()
y_test_np = y_test.to_numpy()

#Importing the pytorch library
#Also importing the 1D convolutional layer and as we are inputting 1D row of data, cant use 2D or 3D
import torch
from torch.nn import Conv1d

#importing max pooling layer
from torch.nn import MaxPool1d

#Import flatten layer
from torch.nn import Flatten

#Importing linear layer
from torch.nn import Linear

#Importing the ReLU activation function
from torch.nn.functional import relu

#Importing the TensorLoader and dataloader libraries from pytorch to work with the housing dataset
from torch.utils.data import DataLoader, TensorDataset

#The class must be a subclass of torch.nn.Module
class CnnRegressor(torch.nn.Module):

#Defining the initialization method
  def __init__(self, batch_size, inputs, outputs):

#Initializing the superclass and storing the parameters
    super(CnnRegressor, self).__init__()
    self.batch_size = batch_size
    self.inputs = inputs
    self.outputs = outputs

#Defining the input layer with the input size, kernel size and output size
    self.input_layer = Conv1d(inputs, batch_size, 1)

    #Defining a max pooling layer with a kernel size
    self.max_pooling_layer = MaxPool1d(1)

#Defining another convolutional layer
    self.conv_layer = Conv1d(batch_size, 128, 1)

#Defining a flatten layer
    self.flatten_layer = Flatten()

#defining a linear layer with inputs and output as the arguments
    self.linear_layer = Linear(128, 64)

#Defining the output layer    
    self.output_layer = Linear(64, outputs)

#Defining a method to feed inputs through the model
  def feed(self, input):

#Reshaping the entry so that it can be fed to the input layer
#Although 1D convolutional layer is used, it expects 3D array to process in a 1D fashion
    input = input.reshape((self.batch_size, self.inputs, 1))

    #Extracting the first layers output and running it through ReLU activation Layer
    output = relu(self.input_layer(input))

    #Getting the output of the max pooling layer
    output = self.max_pooling_layer(output)

    #Getting the output of the 2nd convolution layer and running it through ReLU activation function
    output = relu(self.conv_layer(output))

    #Extracting the output of flatten layer
    output = self.flatten_layer(output)

    #Getting the output of linear layer and then run it through the ReLU activation layer
    output = self.linear_layer(output)

    #Returning the output of the extracted output layer
    output = self.output_layer(output)
    return output

#Importing the Stochastic gradient descent package from pytorch for our optimizer
from torch.optim import SGD

#Importing the mean absolute error loss package from pytorch for our performance measure 
from torch.nn import L1Loss

#Importing the R^2 score package from pytorch's ignite for our score measure 
!pip install pytorch-ignite
from ignite.contrib.metrics.regression.r2_score import R2Score

#Defining batch size that can be optimal as 64
batch_size = 64 

model = CnnRegressor(batch_size, X.shape[1], 1)

model.cuda()

#Creating a method for running the batches of data through the model created 
#Method returns average L1 Loss and R^2 score
def model_loss(model, dataset, train = False, optimizer = None):

  #cycling through the batches and getting the average L1 Loss
  performance = L1Loss()
  score_metric = R2Score()

  avg_loss = 0
  avg_score = 0
  count = 0
  for input, output in iter(dataset):

    #Getting the model's prediction for training dataset
    predictions = model.feed(input)

    #Extracting model's loss
    loss = performance(predictions, output)

    #Getting the model's R^2 Score
    score_metric.update([predictions, output])
    score = score_metric.compute()

    if(train):
      
      #Clearing the error so as to not have it cummulate
      optimizer.zero_grad()

      #Computing the gradients for the optimizer
      loss.backward()

      #Using the optimizer to update model's parameters based on the gradients
      optimizer.step()

    #Storing the loss and updating the counter
    avg_loss += loss.item()
    avg_score += score
    count += 1

  return avg_loss / count, avg_score / count

#Defining the number of epochs
epochs = 10

#Defining the performance measure and optimizer
optimizer = SGD(model.parameters(), lr=1e-5)

#converting the training set into torch variables for our model using the GPU as floats.
#Reshaping to remove the warning that pytorch may provide
inputs = torch.from_numpy(x_train_np).cuda().float()
outputs = torch.from_numpy(y_train_np.reshape(y_train_np.shape[0], 1)).cuda().float()

#Creating dataloader instance to work with our batches
tensor = TensorDataset(inputs, outputs)
loader = DataLoader(tensor, batch_size, shuffle=True, drop_last=True)

#Start the training loop 
for epoch in range(epochs):

  #Cycling through the batches and getting average loss
  avg_loss, avg_r2_score = model_loss(model, loader, train=True, optimizer=optimizer)

  #Outputting the average loss
  print("Epoch " + str(epoch + 1) + ":\n\tLoss = " + str(avg_loss) + "\n\tR^2 Score = " +str(avg_r2_score))

#Testing the model
#Convert the testing set into torch variables for our model using the GPU as floats
inputs = torch.from_numpy(x_test_np).cuda().float()
outputs = torch.from_numpy(y_test_np.reshape(y_test_np.shape[0], 1)).cuda().float()

#Create dataloader instance to work with the batches
tensor = TensorDataset(inputs, outputs)
loader = DataLoader(tensor, batch_size, shuffle=True, drop_last=True)

#output the average performance of the proposed model
avg_loss, avg_r2_score = model_loss(model, loader)
print("The model's L1 loss is: " + str(avg_loss))
print("the model's R^2 Score is: " + str(avg_r2_score))